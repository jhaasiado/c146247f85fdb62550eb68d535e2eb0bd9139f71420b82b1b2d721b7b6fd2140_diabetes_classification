# ğŸ©º Diabetes Prediction Pipeline

This repository implements a reproducible binary classification pipeline to predict diabetes presence from medical measurements (e.g., glucose, BMI, age). It follows an MLOps-oriented workflow with clear preprocessing, training, evaluation, and environment setup.

Dataset: **[Kaggle Diabetes Dataset (Pima Indians)](https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset)**.
Chosen for its popularity, reproducibility, and simplicity to showcase a **production-driven ML pipeline**.

---

## ğŸ“‚ Folder Structure

```
.
â”œâ”€â”€ main.py                  # Orchestrates preprocessing â†’ training â†’ evaluation (with logging)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion.py    # Fetch dataset via kagglehub and save CSVs
â”‚   â”œâ”€â”€ data_preprocessing.py# Robust scaling + train/test split
â”‚   â”œâ”€â”€ training.py          # Trains KNN, Logistic Regression, Random Forest, XGBoost
â”‚   â”œâ”€â”€ evaluation.py        # Evaluates models, saves metrics/reports
â”‚   â”œâ”€â”€ utils.py             # Shared helpers (ensure_dir)
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # (Optional) Manually downloaded CSVs
â”‚   â””â”€â”€ preprocessed/        # Scaled/cleaned CSVs generated by pipeline
â”œâ”€â”€ models/                  # Trained model pickles
â”œâ”€â”€ results/                 # Metrics + text reports
â”œâ”€â”€ notebooks/               # EDA + reference notebook
â”œâ”€â”€ pyproject.toml           # Project config + dependencies
â”œâ”€â”€ .pre-commit-config.yaml  # Linting/formatting/security hooks
â””â”€â”€ requirements.txt         # Exported for pip-based installs
```

---
## âš™ï¸ Setup Instructions

**Prerequisites**: Python 3.12, Git, [UV](https://docs.astral.sh/uv/).

1. Clone the repo and enter project folder.
2. Verify Python version (`.python-version` â†’ 3.12).
3. Install dependencies + set up venv:
   ```bash
   uv sync
   ```
4. Run the pipeline with logging:
   ```bash
   uv run python main.py
   ```
5. (Optional, Windows low disk cache issue):
   ```powershell
   Set-Item -Path Env:UV_CACHE_DIR -Value ".uv-cache"
   ```

---

## ğŸ“Š Running Visualizations and References

- Reference notebook: `notebooks/diabetes-eda-ml-prediction.ipynb`
- Preprocessing uses Robust scaling consistent with the pipeline.
- When main.py is ran, it shows the evaluation metrics per model in the CLI.

---

## ğŸ” Outputs

Running the pipeline produces:
- Preprocessed data: `data/preprocessed/diabetes_scaled.csv`
- Trained models: `models/model_<name>.pkl`
- Evaluation results:
  - `results/metrics.json`
  - `results/results.txt` (classification reports)

---

## ğŸ§¹ Pre-commit Hooks

Ensures clean, secure, consistent code before commits:
- Hygiene: trailing-whitespace, end-of-file-fixer, check-yaml/json/toml
- Lint/format: ruff --fix, black
- Security: bandit, detect-secrets (baseline at `.secrets.baseline`)
- Spelling: codespell
- Notebook cleanup: nbstripout

Run manually: `uv run pre-commit run --all-files`

## âš”ï¸ Challenges

- Disk space and UV cache: During setup, large wheels (numpy, statsmodels, pywin32) failed to extract due to low space in the default UV cache location. Fix: set `UV_CACHE_DIR=.uv-cache` and reâ€‘run `uv sync` so UV uses a project-local cache.

- Pre-commit and repo hygiene: Early imports failed while wiring the pipeline; adding `src/__init__.py` fixed package imports for `main.py`. Large generated data and `.pkl` model files exceeded pre-commit size thresholds, so theyâ€™re ignored via `.gitignore` and excluded in pre-commit.

--

## ğŸ” Reproducibility Notes

- Python pinned to 3.12
- Dependencies locked via `uv.lock`
- Export `requirements.txt`: `uv export --no-hashes -o requirements.txt`
