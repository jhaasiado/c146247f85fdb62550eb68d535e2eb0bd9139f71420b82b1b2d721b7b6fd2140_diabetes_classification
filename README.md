# Diabetes Prediction Pipeline

This repository implements a reproducible binary classification pipeline to predict diabetes presence from medical measurements (e.g., glucose, BMI, age). It follows an MLOps‑oriented workflow with clear preprocessing, training, evaluation, and environment setup.

Dataset: Kaggle “Diabetes Dataset (Pima Indians)” — https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset

---

## Folder Structure

```
.
├── main.py                   # Orchestrates preprocessing → training → evaluation (with logging)
├── src/
│   ├── data_ingestion.py     # Fetch dataset via kagglehub and save CSVs
│   ├── data_preprocessing.py # Robust scaling + train/test split
│   ├── training.py           # Trains KNN, Logistic Regression, Random Forest, XGBoost
│   ├── evaluation.py         # Evaluates models, saves metrics/reports
│   ├── utils.py              # Shared helpers (ensure_dir)
│   └── __init__.py
├── data/
│   ├── raw/                  # (Optional) Manually downloaded CSVs
│   └── preprocessed/         # Scaled/cleaned CSVs generated by pipeline
├── models/                   # Trained model pickles
├── results/                  # Metrics + text reports
├── deploy/
│   ├── docker/               # Dockerfiles and build context for container images
│   └── airflow/
│       ├── dags/             # Airflow DAGs (workflow orchestration scripts)
│       └── logs/             # Airflow logs (placeholder; managed by Airflow)
├── config/
│   └── airflow/              # Scheduler/webserver configs (or env-based)
├── notebooks/                # EDA + reference notebook
├── pyproject.toml            # Project config + dependencies
├── .pre-commit-config.yaml   # Linting/formatting/security hooks
└── requirements.txt          # Exported for pip-based installs
```

Why this structure for containerization and orchestration?
- Isolating DAGs in `deploy/airflow/dags/` ensures modularity, allowing independent testing of workflow tasks without touching core ML code.
- A dedicated `deploy/docker/` context keeps container build artifacts separate from source, reducing image bloat and avoiding accidental inclusion of local caches.
- Separating runtime configs under `config/airflow/` cleanly distinguishes operational settings (scheduler/webserver) from application logic, simplifying environment‑specific overrides.

---

## Setup Instructions

- Prerequisites: Python 3.12, Git, and UV (https://docs.astral.sh/uv/)
- Steps:
  1) Ensure `.python-version` contains `3.12`.
  2) Install dependencies and create venv: `uv sync`
  3) Run the full pipeline with logging: `uv run python main.py`
  4) Optional (Windows low disk cache): `Set-Item -Path Env:UV_CACHE_DIR -Value ".uv-cache"`

---

## Running Visualizations

- Reference notebook: `notebooks/diabetes-eda-ml-prediction.ipynb`
- Preprocessing uses Robust scaling consistent with the pipeline.

---

## Outputs

Running the pipeline produces:
- Preprocessed data: `data/preprocessed/diabetes_scaled.csv`
- Trained models: `models/model_<name>.pkl`
- Evaluation results:
  - `results/metrics.json`
  - `results/results.txt` (classification reports)

---

## Pre-commit Hooks

Ensures clean, secure, consistent code before commits:
- Hygiene: trailing-whitespace, end-of-file-fixer, check-yaml/json/toml
- Lint/format: ruff --fix, black
- Security: bandit, detect-secrets (baseline at `.secrets.baseline`)
- Spelling: codespell
- Notebook cleanup: nbstripout

Run manually: `uv run pre-commit run --all-files`

---

## Reproducibility Notes

- Python pinned to 3.12
- Dependencies locked via `uv.lock`
- Export `requirements.txt`: `uv export --no-hashes -o requirements.txt`

---

## Challenges

- Disk space and UV cache: During setup, large wheels (numpy, statsmodels, pywin32) failed to extract due to low space in the default UV cache location. Fix: set `UV_CACHE_DIR=.uv-cache` and re‑run `uv sync` so UV uses a project‑local cache.

- Pre-commit and repo hygiene: Early imports failed while wiring the pipeline; adding `src/__init__.py` fixed package imports for `main.py`. Large generated data and `.pkl` model files exceeded pre-commit size thresholds, so they’re ignored via `.gitignore` and excluded in pre-commit.
