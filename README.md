# Diabetes Prediction Pipeline

This repository implements a reproducible binary classification pipeline to predict diabetes presence from medical measurements (e.g., glucose, BMI, age). It follows an MLOps‑oriented workflow with clear preprocessing, training, evaluation, and environment setup.

Dataset: Kaggle “Diabetes Dataset (Pima Indians)” — https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset

---

## Folder Structure

```
.
├── main.py                   # Orchestrates preprocessing → training → evaluation (with logging)
├── src/
│   ├── data_ingestion.py     # Fetch dataset via kagglehub and save CSVs
│   ├── data_preprocessing.py # Robust scaling + train/test split
│   ├── training.py           # Trains KNN, Logistic Regression, Random Forest, XGBoost
│   ├── evaluation.py         # Evaluates models, saves metrics/reports
│   ├── utils.py              # Shared helpers (ensure_dir)
│   └── __init__.py
├── data/
│   ├── raw/                  # (Optional) Manually downloaded CSVs
│   └── preprocessed/         # Scaled/cleaned CSVs generated by pipeline
├── models/                   # Trained model pickles
├── results/                  # Metrics + text reports
├── deploy/
│   └── airflow/
│       ├── dags/             # Airflow DAGs (workflow orchestration scripts)
│       └── logs/             # Airflow logs (placeholder; managed by Airflow)
├── config/
│   └── airflow/              # Scheduler/webserver configs (or env-based)
├── notebooks/                # EDA + reference notebook
├── pyproject.toml            # Project config + dependencies
├── .pre-commit-config.yaml   # Linting/formatting/security hooks
└── requirements.txt          # Exported for pip-based installs
```

Why this structure for containerization and orchestration?
- Isolating DAGs in `deploy/airflow/dags/` ensures modularity, allowing independent testing of workflow tasks without touching core ML code.
- A dedicated `deploy/docker/` context keeps container build artifacts separate from source, reducing image bloat and avoiding accidental inclusion of local caches.
- Separating runtime configs under `config/airflow/` cleanly distinguishes operational settings (scheduler/webserver) from application logic, simplifying environment‑specific overrides.

---

## Setup Instructions

- Prerequisites: Python 3.12, Git, and UV (https://docs.astral.sh/uv/)
- Steps:
  1) Ensure `.python-version` contains `3.12`.
  2) Install dependencies and create venv: `uv sync`
  3) Run the full pipeline with logging: `uv run python main.py`
  4) Optional (Windows low disk cache): `Set-Item -Path Env:UV_CACHE_DIR -Value ".uv-cache"`

- Containerization and Airflow (Compose):
  - Verify Docker: `docker --version` (install from https://docs.docker.com/get-docker/)
  - Pull Airflow image: `docker pull apache/airflow:2.9.3`
  - Navigate to Airflow compose dir: `cd deploy/airflow`
  - Set Airflow UID (Linux/WSL, recommended): `export AIRFLOW_UID=$(id -u)`
  - Initialize Airflow DB and admin user:
    - Docker Compose V2: `docker compose up airflow-init`
  - Start services (webserver, scheduler, postgres):
    - `docker compose up -d`
  - Access UI: http://localhost:8080 (user: `airflow`, password: `airflow`)
  - DAGs path mounted from: `deploy/airflow/dags`
  - Project mounted at: `/opt/airflow/project` for operators to import code

---

## Docker Setup (App Image)

- Dockerfile: `docker/Dockerfile` uses a multi-stage build on `python:3.12-slim`.
  - Stage 1 (builder) installs `uv` and syncs dependencies into a local `.venv` using `pyproject.toml` and `uv.lock` if present.
  - Stage 2 (runtime) copies only the `.venv`, `src/`, and `main.py`, minimizing image size and layers for faster pulls in CI/CD.
  - Entry point runs `python -m src.run_pipeline`, which delegates to `main.main()`.

- Build the image:
  - `docker build -f docker/Dockerfile -t diabetes-pipeline:latest .`

- Run the container (mount local data and outputs):
  - Windows (PowerShell):
    - `docker run --rm -v ${PWD}\data:/app/data -v ${PWD}\models:/app/models -v ${PWD}\results:/app/results diabetes-pipeline:latest`
  - Linux/macOS:
    - `docker run --rm -v "$(pwd)/data:/app/data" -v "$(pwd)/models:/app/models" -v "$(pwd)/results:/app/results" diabetes-pipeline:latest`

- What runs:
  - The container executes the full pipeline (preprocess → train → evaluate) reproducibly, writing outputs to the mounted `models/` and `results/` directories.

---

## Running Visualizations

- Reference notebook: `notebooks/diabetes-eda-ml-prediction.ipynb`
- Preprocessing uses Robust scaling consistent with the pipeline.

---

## Outputs

Running the pipeline produces:
- Preprocessed data: `data/preprocessed/diabetes_scaled.csv`
- Trained models: `models/model_<name>.pkl`
- Evaluation results:
  - `results/metrics.json`
  - `results/results.txt` (classification reports)

---

## Pre-commit Hooks

Ensures clean, secure, consistent code before commits:
- Hygiene: trailing-whitespace, end-of-file-fixer, check-yaml/json/toml
- Lint/format: ruff --fix, black
- Security: bandit, detect-secrets (baseline at `.secrets.baseline`)
- Spelling: codespell
- Notebook cleanup: nbstripout

Run manually: `uv run pre-commit run --all-files`

---

## Reproducibility Notes

- Python pinned to 3.12
- Dependencies locked via `uv.lock`
- Export `requirements.txt`: `uv export --no-hashes -o requirements.txt`

---

## Challenges

- Disk space and UV cache: During setup, large wheels (numpy, statsmodels, pywin32) failed to extract due to low space in the default UV cache location. Fix: set `UV_CACHE_DIR=.uv-cache` and re‑run `uv sync` so UV uses a project‑local cache.

- Pre-commit and repo hygiene: Early imports failed while wiring the pipeline; adding `src/__init__.py` fixed package imports for `main.py`. Large generated data and `.pkl` model files exceeded pre-commit size thresholds, so they’re ignored via `.gitignore` and excluded in pre-commit.
