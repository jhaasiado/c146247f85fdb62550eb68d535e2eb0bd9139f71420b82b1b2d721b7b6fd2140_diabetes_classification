# 🩺 Diabetes Prediction Pipeline

A reproducible **binary classification pipeline** to predict diabetes presence based on medical measurements (glucose, BMI, age, etc.).
This project demonstrates an **MLOps-oriented workflow** with preprocessing, training, evaluation, and deployment readiness.

Dataset: **[Kaggle Diabetes Dataset (Pima Indians)](https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset)**.
Chosen for its popularity, reproducibility, and simplicity to showcase a **production-driven ML pipeline**.

---

## 📂 Folder Structure

```
.
├── main.py                 # Orchestrates preprocessing → training → evaluation
├── src/
│   ├── data_ingestion.py   # Fetches dataset (via kagglehub) & saves preprocessed CSVs
│   ├── data_preprocessing.py # Applies Robust scaling + train/test split
│   ├── training.py         # Trains Logistic Regression, KNN, RF, XGBoost
│   ├── evaluation.py       # Evaluates models, saves metrics/reports
│   └── __init__.py
├── data/
│   ├── raw/                # (Optional) Manually downloaded CSVs
│   └── preprocessed/       # Scaled/cleaned CSVs generated by pipeline
├── models/                 # Trained model pickles
├── results/                # Metrics + text reports
├── notebooks/              # EDA + reference notebook
├── pyproject.toml          # Project config + dependencies
├── .pre-commit-config.yaml # Linting/formatting/security hooks
└── requirements.txt        # Exported for pip-based installs
```

---

## ⚙️ Setup Instructions

**Prerequisites**: Python 3.12, Git, [UV](https://docs.astral.sh/uv/).

1. Clone the repo and enter project folder.
2. Verify Python version (`.python-version` → 3.12).
3. Install dependencies + set up venv:
   ```bash
   uv sync
   ```
4. Run the pipeline with logging:
   ```bash
   uv run python main.py
   ```
5. (Optional, Windows low disk cache issue):
   ```powershell
   Set-Item -Path Env:UV_CACHE_DIR -Value ".uv-cache"
   ```

---

## 📊 Running Visualizations

- Reference notebook: `notebooks/diabetes-eda-ml-prediction.ipynb`
- Preprocessing (`src/data_preprocessing.py`) reuses the same scaling as in the notebook

---

## 🔍 Outputs

Running the pipeline produces:

- Preprocessed data → `data/preprocessed/diabetes_scaled.csv`
- Trained models → `models/model_<name>.pkl`
- Evaluation results →
  - `results/metrics.json`
  - `results/results.txt` (classification reports)
- When `main.py` is ran, it will also print out in the CLI the classification report generated per model.

---

## 🧹 Pre-commit Hooks

Ensures **clean, secure, consistent code** before commits:

- Hygiene: `trailing-whitespace`, `end-of-file-fixer`, `check-yaml/json/toml`
- Lint/format: `ruff --fix`, `black`
- Security: `bandit`, `detect-secrets`
- Spelling: `codespell`
- Notebook cleanup: `nbstripout`

Run manually:
```bash
uv run pre-commit run --all-files
```

---

## 🔁 Reproducibility Notes

- Python pinned to **3.12**
- Dependencies locked via `uv.lock`
- Exported `requirements.txt` for `pip install -r requirements.txt`

---

## Challenges

The first major challenge on this assignment is the lack of disk space during installation of UV. The libraries that I am supposed to install in the environment and extracting large wheels (numpy, statsmodels, pywin32) was not proceeding due to shortage of disk space. In order to fix this, I had to set UV_CACHE_DIR=.uv-cache to keep cache in project and retried sync.

The second one is on the pre-commit hooks where when I was setting up the src files, I had to refactor some of the codes to ensure that the main.py pipeline and imports are working. Initially, I had problems with imports but were able to resolve it by adding init.py under src. Some problem also include committing data and .pkl files that exceed size threshold. I had to ensure that they are part of the .gitignore.
