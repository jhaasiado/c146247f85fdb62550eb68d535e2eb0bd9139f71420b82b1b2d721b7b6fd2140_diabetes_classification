# ğŸ©º Diabetes Prediction Pipeline

A reproducible **binary classification pipeline** to predict diabetes presence based on medical measurements (glucose, BMI, age, etc.).
This project demonstrates an **MLOps-oriented workflow** with preprocessing, training, evaluation, and deployment readiness.

Dataset: **[Kaggle Diabetes Dataset (Pima Indians)](https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset)**.
Chosen for its popularity, reproducibility, and simplicity to showcase a **production-driven ML pipeline**.

---

## ğŸ“‚ Folder Structure

```
.
â”œâ”€â”€ main.py                 # Orchestrates preprocessing â†’ training â†’ evaluation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion.py   # Fetches dataset (via kagglehub) & saves preprocessed CSVs
â”‚   â”œâ”€â”€ data_preprocessing.py # Applies Robust scaling + train/test split
â”‚   â”œâ”€â”€ training.py         # Trains Logistic Regression, KNN, RF, XGBoost
â”‚   â”œâ”€â”€ evaluation.py       # Evaluates models, saves metrics/reports
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # (Optional) Manually downloaded CSVs
â”‚   â””â”€â”€ preprocessed/       # Scaled/cleaned CSVs generated by pipeline
â”œâ”€â”€ models/                 # Trained model pickles
â”œâ”€â”€ results/                # Metrics + text reports
â”œâ”€â”€ notebooks/              # EDA + reference notebook
â”œâ”€â”€ pyproject.toml          # Project config + dependencies
â”œâ”€â”€ .pre-commit-config.yaml # Linting/formatting/security hooks
â””â”€â”€ requirements.txt        # Exported for pip-based installs
```

---

## âš™ï¸ Setup Instructions

**Prerequisites**: Python 3.12, Git, [UV](https://docs.astral.sh/uv/).

1. Clone the repo and enter project folder.
2. Verify Python version (`.python-version` â†’ 3.12).
3. Install dependencies + set up venv:
   ```bash
   uv sync
   ```
4. Run the pipeline with logging:
   ```bash
   uv run python main.py
   ```
5. (Optional, Windows low disk cache issue):
   ```powershell
   Set-Item -Path Env:UV_CACHE_DIR -Value ".uv-cache"
   ```

---

## ğŸ“Š Running Visualizations

- Reference notebook: `notebooks/diabetes-eda-ml-prediction.ipynb`
- Preprocessing (`src/data_preprocessing.py`) reuses the same scaling as in the notebook

---

## ğŸ” Outputs

Running the pipeline produces:

- Preprocessed data â†’ `data/preprocessed/diabetes_scaled.csv`
- Trained models â†’ `models/model_<name>.pkl`
- Evaluation results â†’
  - `results/metrics.json`
  - `results/results.txt` (classification reports)
- When `main.py` is ran, it will also print out in the CLI the classification report generated per model.

---

## ğŸ§¹ Pre-commit Hooks

Ensures **clean, secure, consistent code** before commits:

- Hygiene: `trailing-whitespace`, `end-of-file-fixer`, `check-yaml/json/toml`
- Lint/format: `ruff --fix`, `black`
- Security: `bandit`, `detect-secrets`
- Spelling: `codespell`
- Notebook cleanup: `nbstripout`

Run manually:
```bash
uv run pre-commit run --all-files
```

---

## ğŸ” Reproducibility Notes

- Python pinned to **3.12**
- Dependencies locked via `uv.lock`
- Exported `requirements.txt` for `pip install -r requirements.txt`

---

## Challenges

The first major challenge on this assignment is the lack of disk space during installation of UV. The libraries that I am supposed to install in the environment and extracting large wheels (numpy, statsmodels, pywin32) was not proceeding due to shortage of disk space. In order to fix this, I had to set UV_CACHE_DIR=.uv-cache to keep cache in project and retried sync.

The second one is on the pre-commit hooks where when I was setting up the src files, I had to refactor some of the codes to ensure that the main.py pipeline and imports are working. Initially, I had problems with imports but were able to resolve it by adding init.py under src. Some problem also include committing data and .pkl files that exceed size threshold. I had to ensure that they are part of the .gitignore.
